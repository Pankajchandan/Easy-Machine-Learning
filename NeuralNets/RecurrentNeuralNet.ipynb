{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training data\n",
    "The network need a big txt file as an input.\n",
    "The content of the file will be used to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 137629 chars, 81 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('kafka.txt', 'r').read()\n",
    "## this will return a set of unique chars\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d chars, %d unique' % (data_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode/Decode char/vector\n",
    "Neural networks operate on vectors (a vector is an array of float) So we need a way to encode and decode a char as a vector.\n",
    "We'll count the number of unique chars (vocab_size). That will be the size of the vector. The vector contains only zero exept for the position of the char wherae the value is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'C': 31, '!': 3, ' ': 4, '\"': 5, '%': 6, '$': 7, \"'\": 8, ')': 9, '(': 10, '*': 11, '-': 12, ',': 13, '/': 2, '.': 15, '1': 16, '0': 17, '3': 18, '2': 19, '5': 20, '4': 21, '7': 22, '6': 23, '9': 24, '8': 25, ';': 26, ':': 27, '?': 28, 'A': 29, '@': 30, '\\xc3': 1, 'B': 32, 'E': 33, 'D': 34, 'G': 35, 'F': 36, 'I': 37, 'H': 38, 'K': 39, 'J': 40, 'M': 41, 'L': 42, 'O': 43, 'N': 44, 'Q': 45, 'P': 46, 'S': 47, 'R': 48, 'U': 49, 'T': 50, 'W': 51, 'V': 52, 'Y': 53, 'X': 54, 'd': 59, 'a': 55, 'c': 56, 'b': 57, 'e': 58, '\\xa7': 14, 'g': 60, 'f': 61, 'i': 62, 'h': 63, 'k': 64, 'j': 65, 'm': 66, 'l': 67, 'o': 68, 'n': 69, 'q': 70, 'p': 71, 's': 72, 'r': 73, 'u': 74, 't': 75, 'w': 76, 'v': 77, 'y': 78, 'x': 79, 'z': 80}\n",
      "{0: '\\n', 1: '\\xc3', 2: '/', 3: '!', 4: ' ', 5: '\"', 6: '%', 7: '$', 8: \"'\", 9: ')', 10: '(', 11: '*', 12: '-', 13: ',', 14: '\\xa7', 15: '.', 16: '1', 17: '0', 18: '3', 19: '2', 20: '5', 21: '4', 22: '7', 23: '6', 24: '9', 25: '8', 26: ';', 27: ':', 28: '?', 29: 'A', 30: '@', 31: 'C', 32: 'B', 33: 'E', 34: 'D', 35: 'G', 36: 'F', 37: 'I', 38: 'H', 39: 'K', 40: 'J', 41: 'M', 42: 'L', 43: 'O', 44: 'N', 45: 'Q', 46: 'P', 47: 'S', 48: 'R', 49: 'U', 50: 'T', 51: 'W', 52: 'V', 53: 'Y', 54: 'X', 55: 'a', 56: 'c', 57: 'b', 58: 'e', 59: 'd', 60: 'g', 61: 'f', 62: 'i', 63: 'h', 64: 'k', 65: 'j', 66: 'm', 67: 'l', 68: 'o', 69: 'n', 70: 'q', 71: 'p', 72: 's', 73: 'r', 74: 'u', 75: 't', 76: 'w', 77: 'v', 78: 'y', 79: 'x', 80: 'z'}\n"
     ]
    }
   ],
   "source": [
    "## this are 2 dicts to convert a char to int and int to char\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print char_to_ix\n",
    "print ix_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary defined above allosw us to create a vector of size 61 instead of 256.\n",
    "Here and exemple of the char 'a'\n",
    "The vector contains only zeros, except at position char_to_ix['a'] where we put a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print vector_for_char_a.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model parameters\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters are:\n",
    "* Wxh are parameters to connect a vector that contain one input to the hidden layer.\n",
    "* Whh are parameters to connect the hidden layer to itself. This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
    "* Why are parameters to connect the hidden layer to the output\n",
    "* bh contains the hidden bias\n",
    "* by contains the output bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the loss function\n",
    "The loss is a key concept in all neural networks training. It is a value that describe how good is our model.\n",
    "The smaller the loss, the better our model is.\n",
    "(A good model is a model where the predicted output is close to the training output)\n",
    "During the training phase we want to minimize the loss.\n",
    "The loss function calculates the loss but also the gradients (by backward pass):\n",
    "* It perform a forward pass: calculate the next char given a char from the training set.\n",
    "* It calculate the loss by comparing the predicted char to the target char. (The target char is the input following char in the tranning set)\n",
    "* It calculates the backward pass to calculate the gradients\n",
    "\n",
    "This function take as input:\n",
    "* a list of input char\n",
    "* a list of target char\n",
    "* and the previous hidden state\n",
    "\n",
    "This function outputs:\n",
    "* the loss\n",
    "* the gradient for each parameters between layers\n",
    "* the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \n",
    "  \"\"\"\n",
    "  #stores our inputs, hidden states, outputs, and probability values\n",
    "    xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "      # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "      # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "      # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "      # to calculate the hidden state at t = 0\n",
    "      # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "      # ps will take the ys and convert them to normalized probab for chars\n",
    "      # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "      # -1 as  a list index would wrap around to the final element\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "      #init with previous hidden state\n",
    "      # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "      # We don't want hs[-1] to automatically change if hprev is changed\n",
    "    hs[-1] = np.copy(hprev)\n",
    "      #init loss as 0\n",
    "    loss = 0\n",
    "    \n",
    "  # forward pass                                                                                                                                                                              \n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "        xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)  \n",
    "        \n",
    "    # backward pass: compute gradients going backwards    \n",
    "    #initalize vectors for gradient values for each set of weights \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        #output probabilities\n",
    "        dy = np.copy(ps[t])\n",
    "        #derive our first gradient\n",
    "        dy[targets[t]] -= 1 # backprop into y  \n",
    "        #compute output gradient -  output times hidden states transpose\n",
    "        #When we apply the transpose weight matrix,  \n",
    "        #we can think intuitively of this as moving the error backward\n",
    "        #through the network, giving us some sort of measure of the error \n",
    "        #at the output of the lth layer. \n",
    "        #output gradient\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        #derivative of output bias\n",
    "        dby += dy\n",
    "        #backpropagate!\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "        dbh += dhraw #derivative of hidden bias\n",
    "        dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Whh.T, dhraw) \n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients      \n",
    "        \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
