{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be using a recurrent neural network to model binary addition. \n",
    "Binary addition moves from right to left, where we try to predict the number beneath the line given the numbers above the line. We want the neural network to move along the binary sequences and remember when it has carried the 1 and when it hasn't, so that it can make the correct prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy, numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training dataset generation\n",
    "int2binary = {}\n",
    "binary_dim = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input variables\n",
    "alpha = 0.1 ##learning rate\n",
    "##We are adding two numbers together, so we'll be feeding in two-bit strings one character at the time each. \n",
    "##Thus, we need to have two inputs to the network (one for each of the numbers being added).\n",
    "input_dim = 2 \n",
    "##This is the size of the hidden layer that will be storing our carry bit\n",
    "hidden_dim = 16\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize neural network weights\n",
    "\n",
    "##This is the matrix of weights that connects our input layer and our hidden layer. \n",
    "##Thus, it has \"input_dim\" rows and \"hidden_dim\" columns. (2 x 16)\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
    "##This is the matrix of weights that connects the hidden layer to the output layer. \n",
    "##Thus, it has \"hidden_dim\" rows and \"output_dim\" columns. (16 x 1 unless you change it).\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
    "##This is the matrix of weights that connects the hidden layer in the previous time-step to the hidden layer\n",
    "##in the current timestep. It also connects the hidden layer in the current timestep to the hidden layer in \n",
    "##the next timestep . Thus, it has the dimensionality of \"hidden_dim\" rows and \"hidden_dim\" columns. (16 x 16).\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
    "\n",
    "##These store the weight updates that we would like to make for each of the weight matrices. \n",
    "##After we've accumulated several weight updates, we'll actually update the matrices\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[[ 4.43556853]]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 0 1 1 1 0]\n",
      "84 + 26 = 0\n",
      "------------\n",
      "Error:[[ 4.04943453]]\n",
      "Pred:[1 0 1 1 0 1 0 1]\n",
      "True:[0 1 0 1 1 1 1 0]\n",
      "84 + 10 = 181\n",
      "------------\n",
      "Error:[[ 3.81363021]]\n",
      "Pred:[1 1 1 1 1 1 1 1]\n",
      "True:[0 1 1 0 1 0 1 1]\n",
      "60 + 47 = 255\n",
      "------------\n",
      "Error:[[ 3.99675767]]\n",
      "Pred:[1 0 0 0 0 1 1 0]\n",
      "True:[1 1 0 0 0 0 0 0]\n",
      "99 + 93 = 134\n",
      "------------\n",
      "Error:[[ 3.1408269]]\n",
      "Pred:[0 1 1 0 1 1 1 1]\n",
      "True:[0 1 1 0 1 0 1 1]\n",
      "59 + 48 = 111\n",
      "------------\n",
      "Error:[[ 2.51276591]]\n",
      "Pred:[1 0 0 0 1 0 1 1]\n",
      "True:[1 0 0 0 1 0 1 1]\n",
      "36 + 103 = 139\n",
      "------------\n",
      "Error:[[ 1.1066054]]\n",
      "Pred:[0 1 0 0 1 0 1 0]\n",
      "True:[0 1 0 0 1 0 1 0]\n",
      "28 + 46 = 74\n",
      "------------\n",
      "Error:[[ 1.12224361]]\n",
      "Pred:[0 1 0 0 0 0 0 1]\n",
      "True:[0 1 0 0 0 0 0 1]\n",
      "15 + 50 = 65\n",
      "------------\n",
      "Error:[[ 0.7802429]]\n",
      "Pred:[1 0 0 0 0 0 1 0]\n",
      "True:[1 0 0 0 0 0 1 0]\n",
      "47 + 83 = 130\n",
      "------------\n",
      "Error:[[ 0.33935628]]\n",
      "Pred:[0 1 1 0 1 1 0 0]\n",
      "True:[0 1 1 0 1 1 0 0]\n",
      "37 + 71 = 108\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# training logic\n",
    "for j in range(10000):\n",
    "    \n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # binary encoding\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "\n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # where we'll store our best guess (binary encoded)\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    overallError = 0\n",
    "    \n",
    "    layer_2_deltas = list() ## layer2 derivatives\n",
    "    layer_1_values = list() ## layer1 value\n",
    "    layer_1_values.append(np.zeros(hidden_dim)) ## initialize layer1\n",
    "    \n",
    "    # moving along the positions in the binary encoding\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # generate input and output\n",
    "        ##X is the same as \"layer_0\". X is a list of 2 numbers, one from a and one from b. \n",
    "        ##It's indexed according to the \"position\" variable, but we index it in such a way that it goes from \n",
    "        ##right to left. So, when position == 0, this is the farhest bit to the right in \"a\" and the farthest \n",
    "        ##bit to the right in \"b\". When position equals 1, this shifts to the left one bit.\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        ## here magic happens!! the prev hidden layers are dotted with weight matrix at layer 2 and summed\n",
    "        ## with the dot product of input and weight matrix at layer 1\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # did we miss?... if so, by how much?\n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n",
    "        overallError += np.abs(layer_2_error)\n",
    "    \n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(layer_2)\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        ##don't miss this\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        # error at hidden layer\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        # let's update all our weights so we can try again\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    # print out progress\n",
    "    if(j % 1000 == 0):\n",
    "        print \"Error:\" + str(overallError)\n",
    "        print \"Pred:\" + str(d)\n",
    "        print \"True:\" + str(c)\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print str(a_int) + \" + \" + str(b_int) + \" = \" + str(out)\n",
    "        print \"------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
