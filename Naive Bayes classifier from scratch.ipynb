{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intutive intro i found in qoura:\n",
    "\n",
    "Bayesian machine learning allows us to encode our prior beliefs about what those models should look like, independent of what the data tells us. This is especially useful when we don’t have a ton of data to confidently learn our model.\n",
    "\n",
    "A simple example is learning a model of a flipped coin. The model for a particular coin predicts that coin’s probability of landing on heads when it’s flipped — we’ll call that probability the model’s parameter. One way (a non-Bayesian way) to learn this model is to flip the coin 10 times, and set the model’s parameter to be the percentage of flips that were heads. So if it was 5 heads, 5 tails, then the parameter is 50%, if it was 7 heads then the parameter is 70%, etc.\n",
    "\n",
    "One problem with this method is that with limited data (only 10 flips), you’re likely to end up with a parameter that isn’t correct. Fortunately, you know from your experience as a human that most coins are about 50–50. So you can use Bayesian inference to encode this prior knowledge.\n",
    "\n",
    "Doing this allows you to say things like: “I’m 98.4% certain it’s a fair coin, but if it’s not, it’s probably somewhere near a bias of 70–30.” Note that not only have we used our prior belief to get a (hopefully) better answer, we’re now also expressing uncertainty in answering the question of whether it’s a fair coin, rather than sticking to a single answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a more formal way:\n",
    "The Naive Bayes algorithm is an intuitive method that uses the probabilities of each attribute belonging to each class to make a prediction. It is the supervised learning approach you would come up with if you wanted to model a predictive modeling problem probabilistically.\n",
    "\n",
    "Naive bayes simplifies the calculation of probabilities by assuming that the probability of each attribute belonging to a given class value is independent of all other attributes. This is a strong assumption but results in a fast and effective method.\n",
    "\n",
    "The probability of a class value given a value of an attribute is called the conditional probability. By multiplying the conditional probabilities together for each attribute for a given class value, we have a probability of a data instance belonging to that class.\n",
    "\n",
    "To make a prediction we can calculate probabilities of the instance belonging to each class and select the class value with the highest probability.\n",
    "\n",
    "Naive bases is often described using categorical data because it is easy to describe and calculate using ratios. A more useful version of the algorithm for our purposes supports numeric attributes and assumes the values of each numerical attribute are normally distributed (fall somewhere on a bell curve). Again, this is a strong assumption, but still gives robust results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process followed: \n",
    "1. Handle Data: Load the data from CSV file and split it into training and test datasets.\n",
    "2. Summarize Data: summarize the properties in the training dataset so that we can calculate probabilities and make predictions.\n",
    "3. Make a Prediction: Use the summaries of the dataset to generate a single prediction.\n",
    "4. Make Predictions: Generate predictions given a test dataset and a summarized training dataset.\n",
    "5. Evaluate Accuracy: Evaluate the accuracy of predictions made for a test dataset as the percentage correct out of all predictions made.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load data\n",
    "import csv\n",
    "def loadCsv(filename):\n",
    "    lines = csv.reader(open(filename, \"r\"))\n",
    "    dataset = list(lines)\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i] = [float(x) for x in dataset[i]]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file {0} with {1} rows\n"
     ]
    }
   ],
   "source": [
    "filename = 'pima-indians-diabetes.csv'\n",
    "dataset = loadCsv(filename)\n",
    "print('Loaded data file {0} with {1} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset:  768\n"
     ]
    }
   ],
   "source": [
    "print('length of dataset: ',len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##split a loaded dataset into a train and test datasetsPython\n",
    "\n",
    "import random\n",
    "def splitDataset(dataset, splitRatio):\n",
    "    trainSize = int(len(dataset) * splitRatio)\n",
    "    trainSet = []\n",
    "    copy = list(dataset)\n",
    "    while len(trainSet) < trainSize:\n",
    "        index = random.randrange(len(copy))\n",
    "        trainSet.append(copy.pop(index))\n",
    "    return [trainSet, copy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize Data\n",
    "The naive bayes model is comprised of a summary of the data in the training dataset. This summary is then used when making predictions.\n",
    "\n",
    "The summary of the training data collected involves the mean and the standard deviation for each attribute, by class value. For example, if there are two class values and 7 numerical attributes, then we need a mean and standard deviation for each attribute (7) and class value (2) combination, that is 14 attribute summaries.\n",
    "\n",
    "These are required when making predictions to calculate the probability of specific attribute values belonging to each class value.\n",
    "\n",
    "We can break the preparation of this summary data down into the following sub-tasks:\n",
    "\n",
    "1. Separate Data By Class\n",
    "2. Calculate Mean\n",
    "3. Calculate Standard Deviation\n",
    "4. Summarize Dataset\n",
    "5. Summarize Attributes By Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "##separate by class value creates a map of each class value to a list of instances that \n",
    "##belong to that class and sort the entire dataset of instances into the appropriate lists.\n",
    "\n",
    "def separateByClass(dataset):\n",
    "    separated = {}\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        if (vector[-1] not in separated):\n",
    "            separated[vector[-1]] = []\n",
    "        separated[vector[-1]].append(vector)\n",
    "    return separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##calculate mean and std deviation\n",
    "\n",
    "import math\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))\n",
    " \n",
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([pow(x-avg,2) for x in numbers])/float(len(numbers)-1)\n",
    "    return math.sqrt(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##summarize the data\n",
    "\n",
    "def summarize(dataset):\n",
    "    summaries = [(mean(attribute), stdev(attribute)) for attribute in zip(*dataset)]\n",
    "    del summaries[-1]\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## summarize by class\n",
    "def summarizeByClass(dataset):\n",
    "    separated = separateByClass(dataset)\n",
    "    summaries = {}\n",
    "    for classValue, instances in separated.items():\n",
    "        summaries[classValue] = summarize(instances)\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Prediction\n",
    "We are now ready to make predictions using the summaries prepared from our training data. Making predictions involves calculating the probability that a given data instance belongs to each class, then selecting the class with the largest probability as the prediction.\n",
    "\n",
    "We can divide this part into the following tasks:\n",
    "\n",
    "1. Calculate Gaussian Probability Density Function\n",
    "2. Calculate Class Probabilities\n",
    "3. Make a Prediction\n",
    "4. Estimate Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##calculate gaussian probability \n",
    "import math\n",
    "def calculateProbability(x, mean, stdev):\n",
    "    exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n",
    "    return (1 / (math.sqrt(2*math.pi) * stdev)) * exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can calculate the probability of an attribute belonging to a class, we can combine the probabilities of all of the attribute values for a data instance and come up with a probability of the entire data instance belonging to the class.\n",
    "\n",
    "We combine probabilities together by multiplying them. In the calculateClassProbabilities() below, the probability of a given data instance is calculated by multiplying together the attribute probabilities for each class. the result is a map of class values to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateClassProbabilities(summaries, inputVector):\n",
    "    probabilities = {}\n",
    "    for classValue, classSummaries in summaries.items():\n",
    "        probabilities[classValue] = 1\n",
    "        for i in range(len(classSummaries)):\n",
    "            mean, stdev = classSummaries[i]\n",
    "            x = inputVector[i]\n",
    "            probabilities[classValue] *= calculateProbability(x, mean, stdev)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## return the best class probability\n",
    "\n",
    "def predict(summaries, inputVector):\n",
    "    probabilities = calculateClassProbabilities(summaries, inputVector)\n",
    "    bestLabel, bestProb = None, -1\n",
    "    for classValue, probability in probabilities.items():\n",
    "        if bestLabel is None or probability > bestProb:\n",
    "            bestProb = probability\n",
    "            bestLabel = classValue\n",
    "    return bestLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPredictions(summaries, testSet):\n",
    "    predictions = []\n",
    "    for i in range(len(testSet)):\n",
    "        result = predict(summaries, testSet[i])\n",
    "        predictions.append(result)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get accuracy:\n",
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for x in range(len(testSet)):\n",
    "        if testSet[x][-1] == predictions[x]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet))) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    filename = 'pima-indians-diabetes.csv'\n",
    "    splitRatio = 0.67\n",
    "    dataset = loadCsv(filename)\n",
    "    trainingSet, testSet = splitDataset(dataset, splitRatio)\n",
    "    # prepare model\n",
    "    summaries = summarizeByClass(trainingSet)\n",
    "    # test model\n",
    "    predictions = getPredictions(summaries, testSet)\n",
    "    accuracy = getAccuracy(testSet, predictions)\n",
    "    print('Accuracy ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  75.19685039370079\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sumary of what we did:\n",
    "\n",
    "1. separate by class:  this function will take the train set and devide it into a dictionary having n keys, where n is the number of classes. It seggregates the training data according to classes. Returns a dictionary.\n",
    "\n",
    "2. Summarize: It takes m datapoints having x attributes. It returns (mean, stddev) for each attribute. so it returns X number of (mean,stddev) values.\n",
    "\n",
    "3. Summarize by class: it iterates over the dictionary returned by separate by class and calls summarize. So it summarizes on the basis of classes. For n classes it returns a dictionary having n keys and each key will have X number of (mean,stddev) values where X is the number of attributes of a datapoint.\n",
    "\n",
    "4. calculate probability by class: It takes an input datapoint to be predicted. The datapoint has X attributes. It calculates the probability of that attribute in each of the n classes. Now the probability of each class will have X values (because of X attributes). Multiply those X values to get a single probability for each class. The class with highest probability is the winner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
